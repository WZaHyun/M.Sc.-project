term,definition
A/B testing,"A statistical way of comparing two (or more) techniques, typically an incumbentagainst a new rival. A/B testing aims to determine not only which techniqueperforms better but also to understand whether the difference isstatistically significant. A/B testing usually considers only two techniquesusing one measurement, but it can be applied to any finite number of techniquesand measures."
accuracy,The fraction of predictions that aclassification model got right. 
action,"In reinforcement learning, the mechanism by which the agenttransitions between states of theenvironment. The agent chooses the action by using apolicy."
activation function,"A function (for example, ReLU or sigmoid)that takes in the weighted sum of all of the inputs from the previous layerand then generates and passes an output value (typically nonlinear) to the nextlayer."
active learning,"A training approach in which thealgorithm chooses some of the data it learns from. Active learningis particularly valuable when labeled examplesare scarce or expensive to obtain. Instead of blindly seeking a diverserange of labeled examples, an active learning algorithm selectively seeksthe particular range of examples it needs for learning."
AdaGrad,"A sophisticated gradient descent algorithm that rescales thegradients of each parameter, effectively giving each parameteran independent learning rate. For a full explanation, seethis paper."
agent,"In reinforcement learning, the entity that uses a policyto maximize expected return gained from transitioningbetween states of the  environment."
agglomerative clustering,See hierarchical clustering.
AR,Abbreviation for augmented reality.
artificial general intelligence,"A non-human mechanism that demonstrates a broad range of problem solving,creativity, and adaptability. For example, a program demonstrating artificialgeneral intelligence could translate text, compose symphonies, and excel atgames that have not yet been invented."
artificial intelligence,"A non-human program or model that can solve sophisticated tasks. For example,a program or model that translates text or a program or model that identifiesdiseases from radiologic images both exhibit artificial intelligence."
attribute,"Synonym for feature. In fairness, attributes often refer tocharacteristics pertaining to individuals."
AUC (Area under the ROC Curve),An evaluation metric that considers all possibleclassification thresholds.
augmented reality,"A technology that superimposes a computer-generated image on a user's view ofthe real world, thus providing a composite view."
automation bias,"When a human decision maker favors recommendations made by an automateddecision-making system over information made without automation, evenwhen the automated decision-making system makes errors."
average precision,A metric for summarizing the performance of a ranked sequence of results.Average precision is calculated by taking the average of theprecision values for each relevant result (each result inthe ranked list where the recall increases relative to the previous result).
backpropagation,"The primary algorithm for performinggradient descent onneural networks. First, the output valuesof each node are calculated (and cached) in a forward pass.Then, the partial derivativeof the error with respect to each parameter is calculated in a backwardpass through the graph."
bag of words,"A representation of the words in a phrase or passage,irrespective of order. "
baseline,"A model used as a reference point for comparing how well anothermodel (typically, a more complex one) is performing. For example, alogistic regression model might serve as agood baseline for a deep model."
batch,"The set of examples used in one iteration (that is, onegradient update) ofmodel training."
batch normalization,Normalizing the input or output of theactivation functions in ahidden layer.
batch size,"The number of examples in a batch. For example, the batch sizeof SGD is 1, while the batch size ofa mini-batch is usually between 10 and 1000. Batch size isusually fixed during training and inference;however, TensorFlow does permit dynamic batch sizes."
Bayesian neural network,"A probabilistic neural network that accounts foruncertainty in weights and outputs. A standard neural networkregression model typically predicts a scalar value;for example, a model predicts a house priceof 853,000. By contrast, a Bayesian neural network predicts a distribution ofvalues; for example, a model predicts a house price of 853,000 with a standarddeviation of 67,200. A Bayesian neural network relies onBayes' Theoremto calculate uncertainties in weights and predictions. A Bayesian neuralnetwork can be useful when it is important to quantify uncertainty, such as inmodels related to pharmaceuticals. Bayesian neural networks can also helpprevent overfitting."
bias (ethics/fairness),"1. Stereotyping, prejudice or favoritism towards some things, people,or groups over others. These biases can affect collection andinterpretation of data, the design of a system, and how users interactwith a system.  2. Systematic error introduced by a sampling or reporting procedure."
bias (math),An intercept or offset from an origin. Bias (also known as the bias term) is referred to as b or w0 in machine learning models. 
bigram,An N-gram in which N=2.
binary classification,"A type of classification task that outputs oneof two mutually exclusive classes. For example, a machinelearning model that evaluates email messages and outputs either ""spam"" or""not spam"" is a binary classifier."
boosting,A machine learning technique that iteratively combines a set of simple andnot very accurate classifiers (referred to as "weak" classifiers) into aclassifier with high accuracy (a "strong" classifier) byupweighting the examples that the model is currentlymisclassfying.
bounding box,"In an image, the (x, y) coordinates of a rectangle around an area ofinterest."
broadcasting,"Expanding the shape of an operand in a matrix math operation todimensions compatible for that operation. For instance,linear algebra requires that the two operands in a matrix addition operationmust have the same dimensions. Consequently, you can't add a matrix of shape(m, n) to a vector of length n. Broadcasting enables this operation byvirtually expanding the vector of length n to a matrix of shape (m,n) byreplicating the same values down each column."
bucketing,"Converting a (usually continuous) feature intomultiple binary features called buckets or bins, typically based on valuerange. For example, instead of representing temperature as a singlecontinuous floating-point feature, you could chop ranges of temperaturesinto discrete bins. Given temperature data sensitive to a tenth of a degree,all temperatures between 0.0 and 15.0 degrees could be put into one bin,15.1 to 30.0 degrees could be a second bin, and 30.1 to 50.0 degrees couldbe a third bin."
calibration layer,"A post-prediction adjustment, typically to account forprediction bias. The adjusted predictions andprobabilities should match the distribution of an observed set of labels."
candidate generation,"The initial set of recommendations chosen by a recommendation system. Forexample, consider a bookstore that offers 100,000 titles. The candidategeneration phase creates a much smaller list of suitable books for aparticular user, say 500. But even 500 books is way too many to recommendto a user. Subsequent, more expensive, phases of a recommendation system(such as scoring and re-ranking) whittledown those 500 to a much smaller, more useful set of recommendations."
candidate sampling,"A training-time optimization in which a probability is calculated for all thepositive labels, using, for example, softmax,but only for a randomsample of negative labels. For example, if we have an example labeledbeagle and dog candidate sampling computes the predicted probabilitiesand corresponding loss terms for the beagle and dog class outputsin addition to a random subset of the remaining classes(cat, lollipop, fence). The idea is that thenegative classes can learn from less frequentnegative reinforcement as long aspositive classes always get proper positivereinforcement, and this is indeed observed empirically. The motivation forcandidate sampling is a computational efficiency win from not computingpredictions for all negatives."
categorical data,"Features having a discrete set of possible values. For example,consider a categorical feature named house style, which has a discrete set ofthree possible values: Tudor, ranch, colonial. By representing house styleas categorical data, the model can learn the separate impacts of Tudor,ranch, and colonial on house price."
centroid,"The center of a cluster as determined by a k-means ork-median algorithm. For instance, if k is 3,then the k-means or k-median algorithm finds 3 centroids."
centroid-based clustering,A category of clustering algorithms that organizes datainto nonhierarchical clusters. k-means is the most widelyused centroid-based clustering algorithm.
checkpoint,"Data that captures the state of the variables of a model at a particulartime. Checkpoints enable exporting model weights, as wellas performing training across multiple sessions. Checkpoints also enabletraining to continue past errors (for example, job preemption). Note thatthe graph itself is not included in a checkpoint."
class,"One of a set of enumerated target values for a label. For example, in abinary classification model that detectsspam, the two classes are spam and not spam.  In amulti-class classification model thatidentifies dog breeds, the classes would be poodle, beagle, pug, and soon."
classification model,"A type of machine learning model for distinguishing among two or morediscrete classes. For example, a natural language processing classificationmodel could determine whether an input sentence was in French, Spanish,or Italian. Compare with regression model."
classification threshold,"A scalar-value criterion that is applied to a model's predicted score in orderto separate the positive class from the negativeclass.  Used when mappinglogistic regression results tobinary classification. For example, considera logistic regression model that determines the probability of a given emailmessage being spam. If the classification threshold is 0.9, then logisticregression values above 0.9 are classified as spam and those below0.9 are classified as not spam."
class-imbalanced dataset,"A binary classification problem in which thelabels for the two classes have significantly differentfrequencies.  For example, a disease dataset in which 0.0001 of exampleshave positive labels and 0.9999 have negative labels is a class-imbalancedproblem, but a football game predictor in which 0.51 of examples label oneteam winning and 0.49 label the other team winning is not aclass-imbalanced problem."
clipping,"A technique for handling outliers. Specifically, reducingfeature values that are greater than a set maximum value down to that maximumvalue. Also, increasing feature values that are less than a specific minimumvalue up to that minimum value."
Cloud TPU,A specialized hardware accelerator designed to speed up machinelearning workloads on Google Cloud Platform.
clustering,"Grouping related examples, particularly duringunsupervised learning. Once all theexamples are grouped, a human can optionally supply meaning to each cluster."
co-adaptation,"When neurons predict patterns in training data by relyingalmost exclusively on outputs of specific other neurons instead of relying onthe network's behavior as a whole. When the patterns that cause co-adaptionare not present in validation data, then co-adaptation causes overfitting.Dropout regularization reduces co-adaptationbecause dropout ensures neurons cannot rely solely on specific other neurons."
collaborative filtering,Making predictions about the interests of one userbased on the interests of many other users.  Collaborative filteringis often used in recommendation systems.
confirmation bias,"The tendency to search for, interpret, favor, and recall information in away that confirms one's preexisting beliefs or hypotheses.Machine learning developers may inadvertently collect or labeldata in ways that influence an outcome supporting their existingbeliefs.  Confirmation bias is a form of implicit bias."
confusion matrix,"An NxN table that summarizes how successful aclassification model's predictions were; that is,the correlation between the label and the model's classification. One axis ofa confusion matrix is the label that the model predicted, and theother axis is the actual label. N represents the number ofclasses. In a binary classificationproblem, N=2. "
continuous feature,A floating-point feature with an infinite range of possible values.Contrast with discrete feature.
convenience sampling,"Using a dataset not gathered scientifically in order to run quickexperiments. Later on, it's essential to switch to a scientifically gathereddataset."
convergence,"Informally, often refers to a state reached during trainingin which training loss and validation losschange very little or not at allwith each iteration after a certain number of iterations. In other words, amodel reaches convergence when additional training on the current data willnot improve the model. In deep learning, loss valuessometimes stay constant or nearly so for many iterations before finallydescending, temporarily producing a false sense of convergence."
convex function,A function in which the region above the graph of the function is aconvex set.  The prototypical convex function isshaped something like the letter U.  
convex optimization,The process of using mathematical techniques such asgradient descent to findthe minimum of a convex function.A great deal of research in machine learning has focused on formulating variousproblems as convex optimization problems and in solving those problems moreefficiently.
convex set,"A subset of Euclidean space such that a line drawn between any two points in thesubset remains completely within the subset.  For instance, the following twoshapes are convex sets:"
convolution,"In mathematics, casually speaking, a mixture of two functions. In machinelearning, a convolution mixes the convolutional filter and the input matrixin order to train weights."
convolutional filter,"One of the two actors in aconvolutional operation. (The other actoris a slice of an input matrix.) A convolutional filter is a matrix havingthe same rank as the input matrix, but a smaller shape.For example, given a 28x28 input matrix, the filter could be any 2D matrixsmaller than 28x28."
convolutional layer,"A layer of a deep neural network in which aconvolutional filter passes along an inputmatrix.  For example, consider the following 3x3convolutional filter:"
convolutional neural network,A neural network in which at least one layer is aconvolutional layer. A typical convolutionalneural network consists of some combination of the following layers:
cost,Synonym for loss.
counterfactual fairness,"A fairness metric that checks whether a classifier produces the same result for one individual as it does for another individual who is identical to the first, except with respect to one or more sensitive attributes. Evaluating a classifier for counterfactual fairness is one method for surfacing potential sources of bias in a model."
coverage bias,See selection bias.
crash blossom,"A sentence or phrase with an ambiguous meaning.Crash blossoms present a significant problem in naturallanguage understanding.For example, the headline Red Tape Holds Up Skyscraper is acrash blossom because an NLU model could interpret the headline literally orfiguratively."
critic,Synonym for Deep Q-Network.
cross-entropy,A generalization of Log Loss tomulti-class classification problems. Cross-entropyquantifies the difference between two probability distributions.  See alsoperplexity.
cross-validation,A mechanism for estimating how well a model will generalize to new data bytesting the model against one or more non-overlapping data subsets withheldfrom the training set.
custom Estimator,An Estimator that you write yourself by followingthese directions.
data analysis,"Obtaining an understanding of data by considering samples, measurement,and visualization. Data analysis can be particularly useful when adataset is first received, before one builds the first model. It isalso crucial in understanding experiments and debugging problems withthe system."
data augmentation,"Artificially boosting the range and number of training examplesby transforming existing examples to create additional examples. For example,suppose images are one of your features, but your dataset doesn't containenough image examples for the model to learn useful associations. Ideally,you'd add enough labeled images to your dataset to enable yourmodel to train properly. If that's not possible, data augmentation can rotate,stretch, and reflect each image to produce many variants of the originalpicture, possibly yielding enough labeled data to enable excellent training."
DataFrame,"A popular datatype for representing datasets in pandas. ADataFrame is analogous to a table. Each column of the DataFrame has a name (aheader), and each row is identified by a number."
data set or dataset,A collection of examples.
Dataset API (tf.data),"A high-level TensorFlow API for reading data andtransforming it into a form that a machine learning algorithm requires.A tf.data.Dataset object represents a sequence of elements, in whicheach element contains one or more Tensors. A tf.data.Iteratorobject provides access to the elements of a Dataset."
decision boundary,"The separator between classes learned by a model in abinary class ormulti-class classification problems. For example,in the following image representing a binary classification problem,the decision boundary is the frontier between the orange class andthe blue class:"
decision threshold,Synonym for classification threshold.
decision tree,"A model represented as a sequence of branching statements. For example, thefollowing over-simplified decision tree branches a few times topredict the price of a house (in thousands of USD).  According to thisdecision tree, a house larger than 160 square meters, having more than threebedrooms, and built less than 10 years ago would have a predicted price of510 thousand USD."
deep model,A type of neural network containing multiplehidden layers.
deep neural network,Synonym for deep model.
Deep Q-Network (DQN),"In Q-learning, a deep neural networkthat predicts Q-functions."
demographic parity,A fairness metric that is satisfied ifthe results of a model's classification are not dependent on agiven sensitive attribute.
dense feature,"A feature in which most values are non-zero, typicallya Tensor of floating-point values. Contrast withsparse feature."
dense layer,Synonym for fully connected layer.
depth,"The number of layers (including anyembedding layers) in a neural networkthat learn weights. For example, a neural network with 5hidden layers and 1 output layer has a depth of 6."
depthwise separable convolutional neural network (sepCNN),"A convolutional neural networkarchitecture based onInception,but where Inception modules are replaced with depthwise separableconvolutions.  Also known as Xception."
device,"A category of hardware that can run a TensorFlow session, includingCPUs, GPUs, and TPUs."
dimension reduction,"Decreasing the number of dimensions used to represent a particular feature ina feature vector, typically by converting to an embedding."
dimensions,"The number of levels of coordinates in a Tensor. For example:
A scalar has zero dimensions; for example, [""Hello""].
A vector has one dimension; for example, [3, 5, 7, 11].
A matrix has two dimensions; for example, [[2, 4, 18], [5, 7, 14]].
You can uniquely specify a particular cell in a one-dimensional vector with one coordinate; you need two coordinates to uniquely specify a particular cell in a two-dimensional matrix.
The number of entries in a feature vector.
The number of elements in an embedding layer."
discrete feature,"A feature with a finite set of possible values. For example,a feature whose values may only be animal, vegetable, or mineral is adiscrete (or categorical) feature. Contrast withcontinuous feature."
discriminative model,"A model that predicts labels from a set of one or morefeatures. More formally, discriminative models define the conditionalprobability of an output given the features and weights; that is:"
discriminator,A system that determines whether examples are real or fake.
disparate impact,Making decisions about people that impact different populationsubgroups disproportionately. This usually refers to situationswhere an algorithmic decision-making process harms or benefitssome subgroups more than others.
disparate treatment,Factoring subjects' sensitive attributesinto an algorithmic decision-making process such that different subgroupsof people are treated differently.
divisive clustering,See hierarchical clustering.
downsampling,"Reducing the amount of information in a feature in order to train a model more efficiently. For example, before training an image recognition model, downsampling high-resolution images to a lower-resolution format.
Training on a disproportionately low percentage of over-represented class examples in order to improve model training on under-represented classes. For example, in a class-imbalanced dataset, models tend to learn a lot about the majority class and not enough about the minority class. Downsampling helps balance the amount of training on the majority and minority classes."
DQN,Abbreviation for Deep Q-Network.
dropout regularization,"A form of regularization useful in trainingneural networks. Dropout regularization works byremoving a random selection of a fixed number of the units in a networklayer for a single gradient step. The more units dropped out, the strongerthe regularization. This is analogous to training the network to emulatean exponentially large ensemble of smaller networks. For full details, seeDropout: A Simple Way to Prevent Neural Networks fromOverfitting."
dynamic model,"A model that is trained online in a continuouslyupdating fashion.  That is, data is continuously entering the model."
eager execution,"A TensorFlow programming environment in which operationsrun immediately. By contrast, operations called ingraph execution don't run until they are explicitlyevaluated. Eager execution is animperative interface, muchlike the code in most programming languages. Eager execution programs aregenerally far easier to debug than graph execution programs."
early stopping,"A method for regularization that involves endingmodel training before training loss finishes decreasing. In earlystopping, you end model training when the loss on avalidation dataset starts to increase, that is, whengeneralization performance worsens."
embeddings,"A categorical feature represented as a continuous-valued feature.Typically, an embedding is a translation of a high-dimensional vectorinto a low-dimensional space. "
embedding space,"The d-dimensional vector space that features from a higher-dimensionalvector space are mapped to. Ideally, the embedding space contains astructure that yields meaningful mathematical results; for example,in an ideal embedding space, addition and subtraction of embeddingscan solve word analogy tasks."
empirical risk minimization (ERM),Choosing the function that minimizes loss on the training set. Contrastwith structural risk minimization.
ensemble,A merger of the predictions of multiple models. 
environment,"In reinforcement learning, the world that contains the agentand allows the agent to observe that world's state. For example,the represented world can be a game like chess, or a physical world like amaze. When the agent applies an action to the environment,then the environment transitions between states."
episode,"In reinforcement learning, each of the repeated attempts by theagent to learn an environment."
epoch,"A full training pass over the entire dataset such that each example has beenseen once.  Thus, an epoch represents N/batch size trainingiterations, where N is the total number of examples."
epsilon greedy policy,"In reinforcement learning, a policy that either follows arandom policy with epsilon probability or agreedy policy otherwise. For example, if epsilon is0.9, then the policy follows a random policy 90% of the time and a greedypolicy 10% of the time."
equality of opportunity,"A fairness metric that checks whether, for a preferred label (one that confers an advantage or benefit to a person) and a given attribute, a classifier predicts that preferred label equally well for all values of that attribute. In other words, equality of opportunity measures whether the people who should qualify for an opportunity are equally likely to do so regardless of their group membership."
equalized odds,"A fairness metric that checks if, for any particular label and attribute, a classifier predicts that label equally well for all values of that attribute."
Estimator,"An instance of the tf.Estimator class, which encapsulates logic that buildsa TensorFlow graph and runs a TensorFlow session. You may create your owncustom Estimators (as describedhere)or instantiate premade Estimators created byothers."
example,One row of a dataset. An example contains one or more featuresand possibly a label. See alsolabeled example andunlabeled example.
experience replay,"In reinforcement learning, a DQN technique used toreduce temporal correlations in training data. The agentstores state transitions in a replay buffer, and thensamples transitions from the replay buffer to create training data."
experimenter's bias,See confirmation bias.
exploding gradient problem,The tendency for gradients in adeep neural networks (especiallyrecurrent neural networks) to becomesurprisingly steep (high). Steep gradients result in very large updatesto the weights of each node in a deep neural network.
fairness constraint,Applying a constraint to an algorithm to ensure one or more definitions of fairness are satisfied. 
fairness metric,A mathematical definition of “fairness” that is measurable. 
false negative (FN),"An example in which the model mistakenly predicted thenegative class. For example, the modelinferred that a particular email message was not spam(the negative class), but that email message actually was spam."
false positive (FP),"An example in which the model mistakenly predicted thepositive class. For example, the model inferredthat a particular email message was spam (the positive class), but thatemail message was actually not spam."
false positive rate (FPR),The x-axis in an ROC curve. 
feature,An input variable used in making predictions.
Feature column (tf.feature_column),A function that specifies how a model should interpret a particular feature. Alist that collects the output returned by calls to such functions is a requiredparameter to all Estimators constructors.
feature cross,A synthetic feature formed by crossing (taking aCartesianproduct of) individual binary features obtained fromcategorical data or fromcontinuous features via bucketing.Feature crosses help represent nonlinear relationships.
feature engineering,"The process of determining which features might be usefulin training a model, and then converting raw data from log files and othersources into said features. In TensorFlow, feature engineering often meansconverting raw log file entries to tf.Exampleprotocol buffers.  See alsotf.Transform."
feature extraction,"Retrieving intermediate feature representations calculated by an unsupervised or pretrained model (for example, hidden layer values in a neural network) for use in another model as input.
Synonym for feature engineering."
feature set,"The group of features your machine learning model trains on.For example, postal code, property size, and property condition mightcomprise a simple feature set for a model that predicts housing prices."
feature spec,Describes the information required to extract features datafrom the tf.Example protocol buffer. 
feature vector,The list of feature values representing an examplepassed into a model.
federated learning,"A distributed machine learning approach that trainsmachine learning models using decentralizedexamples residing on devices such as smartphones.In federated learning, a subset of devices downloads the current modelfrom a central coordinating server. The devices use the examples storedon the devices to make improvements to the model. The devices then uploadthe model improvements (but not the training examples) to the coordinatingserver, where they are aggregated with other updates to yield an improvedglobal model. After the aggregation, the model updates computed by devicesare no longer needed, and can be discarded."
feedback loop,"In machine learning, a situation in which a model's predictions influence thetraining data for the same model or another model. For example, a model thatrecommends movies will influence the movies that people see, which will theninfluence subsequent movie recommendation models."
feedforward neural network (FFN),"A neural network without cyclic or recursive connections. For example,traditional deep neural networks arefeedforward neural networks. Contrast with recurrent neuralnetworks, which are cyclic."
few-shot learning,"A machine learning approach, often used for object classification,designed to learn effective classifiers from only a small number oftraining examples."
fine tuning,Perform a secondary optimization to adjust the parameters of an alreadytrained model to fit a new problem. Fine tuning oftenrefers to refitting the weights of a trainedunsupervised model to asupervised model.
forget gate,The portion of a Long Short-Term Memorycell that regulates the flow of information through the cell.Forget gates maintain context by deciding which information to discardfrom the cell state.
full softmax,See softmax. Contrast withcandidate sampling.
fully connected layer,A hidden layer in which each node isconnected to every node in the subsequent hidden layer.
GAN,Abbreviation for generative adversarialnetwork.
generalization,"Refers to your model's ability to make correct predictions on new,previously unseen data as opposed to the data used to train the model."
generalization curve,"A loss curve showing both thetraining set and thevalidation set.A generalization curve can help you detect possibleoverfitting.  For example, the followinggeneralization curve suggests overfitting because loss forthe validation set ultimately becomes significantly higherthan for the training set."
generalized linear model,"A generalization of least squares regressionmodels, which are based onGaussiannoise, to othertypes of models based on other types of noise, such asPoisson noiseorcategorical noise."
generative adversarial network (GAN),A system to create new data in which a generator createsdata and a discriminator determines whether thatcreated data is valid or invalid.
generative model,"A generative model can theoretically discern the distribution of examples or particular features in a dataset. That is:
p(examples)
Unsupervised learning models are generative."
generator,The subsystem within a generative adversarialnetworkthat creates new examples.
gradient,"The vector of partial derivatives with respect toall of the independent variables.  In machine learning, the gradient isthe vector of partial derivatives of the model function.  The gradient pointsin the direction of steepest ascent."
gradient clipping,A commonly used mechanism to mitigate theexploding gradient problem by artificiallylimiting (clipping) the maximum value of gradients when usinggradient descent to train a model.
gradient descent,"A technique to minimize loss by computing the gradients ofloss with respect to the model's parameters, conditioned on training data.Informally, gradient descent iteratively adjusts parameters, graduallyfinding the best combination of weights and bias tominimize loss."
graph,"In TensorFlow, a computation specification. Nodes in the graphrepresent operations. Edges are directed and represent passing the resultof an operation (a Tensor) as anoperand to another operation. UseTensorBoard to visualize a graph."
graph execution,A TensorFlow programming environment in which the program first constructsa graph and then executes all or part of that graph. Graphexecution is the default execution mode in TensorFlow 1.x.
greedy policy,"In reinforcement learning, a policy that always chooses theaction with the highest expected return."
ground truth,"The correct answer. Reality. Since reality is often subjective,expert raters typically are the proxy for ground truth."
group attribution bias,"Assuming that what is true for an individual is also true for everyonein that group. The effects of group attribution bias can be exacerbatedif a convenience samplingis used for data collection. In a non-representative sample, attributionsmay be made that do not reflect reality."
hashing,"In machine learning, a mechanism for bucketingcategorical data, particularly when the numberof categories is large, but the number of categories actually appearingin the dataset is comparatively small."
heuristic,"A quick solution to a problem, which may or may not be the best solution.For example, ""With a heuristic, we achieved 86% accuracy. When we switchedto a deep neural network, accuracy went up to 98%."""
hidden layer,"A synthetic layer in a neural network between theinput layer (that is, the features) and theoutput layer (the prediction). Hidden layers typicallycontain an activation function (such asReLU) for training.  A deep neuralnetwork contains more than onehidden layer."
hierarchical clustering,"A category of clustering algorithms that create a treeof clusters. Hierarchical clustering is well-suited to hierarchical data,such as botanical taxonomies. "
hinge loss,"A family of loss functions for classification designed to find the decision boundary as distant as possible from each training example, thus maximizing the margin between examples and the boundary. KSVMs use hinge loss (or a related function, such as squared hinge loss). "
holdout data,Examples intentionally not used ("held out") during training.The validation dataset andtest dataset are examples of holdout data. Holdout datahelps evaluate your model's ability to generalize to data other than thedata it was trained on. The loss on the holdout set provides a betterestimate of the loss on an unseen dataset than does the loss on thetraining set.
hyperparameter,"The ""knobs"" that youtweak during successive runs of training a model. For example,learning rate is a hyperparameter."
hyperplane,"A boundary that separates a space into two subspaces.  For example, a line is ahyperplane in two dimensions and a plane is a hyperplane in three dimensions.More typically in machine learning, a hyperplane is the boundary separating ahigh-dimensional space.  Kernel Support Vector Machines usehyperplanes to separate positive classes from negative classes, often in a veryhigh-dimensional space."
i.i.d.,Abbreviation for independently and identically distributed.
image recognition,"A process that classifies object(s), pattern(s), or concept(s) in an image.Image recognition is also known as image classification."
imbalanced dataset,Synonym for class-imbalanced dataset.
implicit bias,Automatically making an association or assumption based on one’s mentalmodels and memories. 
incompatibility of fairness metrics,"The idea that some notions of fairness are mutually incompatible andcannot be satisfied simultaneously. As a result, there is no singleuniversal metric for quantifying fairnessthat can be applied to all ML problems."
independently and identically distributed (i.i.d),"Data drawn from a distribution that doesn't change, and where each valuedrawn doesn't depend on values that have been drawn previously. An i.i.d.is the ideal gasof machinelearning—a useful mathematical construct but almost never exactly foundin the real world. For example, the distribution of visitors to a web pagemay be i.i.d. over a brief window of time; that is, the distribution doesn'tchange during that brief window and one person's visit is generallyindependent of another's visit. However, if you expand that window of time,seasonal differences in the web page's visitors may appear."
individual fairness,"A fairness metric that checks whether similar individuals are classifiedsimilarly. For example, Brobdingnagian Academy might want to satisfyindividual fairness by ensuring that two students with identical gradesand standardized test scores are equally likely to gain admission."
inference,"In machine learning, often refers to the process of making predictions byapplying the trained model to unlabeled examples.In statistics, inference refers to the process of fitting the parametersof a distribution conditioned on some observed data. (See theWikipedia article on statistical inference.)"
in-group bias,"Showing partiality to one's own group or own characteristics.If testers or raters consist of the machine learning developer's friends,family, or colleagues, then in-group bias may invalidate product testingor the dataset."
input function,"In TensorFlow, a function that returns input data to the training, evaluation,or prediction method of an Estimator.  For example,the training input function returns a batch of featuresand labels from the training set."
input layer,The first layer (the one that receives the input data) ina neural network.
instance,Synonym for example.
interpretability,"The degree to which a model's predictions can be readily explained. Deep modelsare often non-interpretable; that is, a deep model's different layers can behard to decipher. By contrast, linear regression models and widemodels are typically far more interpretable."
inter-rater agreement,"A measurement of how often human raters agree when doing a task.If raters disagree, the task instructions may need to be improved.Also sometimes called inter-annotator agreement orinter-rater reliability.  See alsoCohen'skappa,which is one of the most popular inter-rater agreement measurements."
intersection over union (IoU),"The intersection of two sets divided by their union. In machine-learningimage-detection tasks, IoU is used to measure the accuracy of the model’spredicted bounding box with respect to theground-truth bounding box. In this case, the IoU for thetwo boxes is the ratio between the overlapping area and the total area, andits value ranges from 0 (no overlap of predicted bounding box and ground-truthbounding box) to 1 (predicted bounding box and ground-truth bounding box havethe exact same coordinates)."
IoU,Abbreviation for intersection over union.
item matrix,"In recommendation systems, amatrix of embeddings generated bymatrix factorizationthat holds latent signals about each item.Each row of the item matrix holds the value of a single latentfeature for all items.For example, consider a movie recommendation system. Each columnin the item matrix represents a single movie. The latent signalsmight represent genres, or might be harder-to-interpretsignals that involve complex interactions among genre, stars,movie age, or other factors."
items,"In a recommendation system, the entities thata system recommends. For example, videos are the items that a video storerecommends, while books are the items that a bookstore recommends."
iteration,A single update of a model's weights during training.  An iterationconsists of computing the gradients of the parameters with respect to theloss on a single batch of data.
Keras,"A popular Python machine learning API.Kerasruns onseveral deep learning frameworks, including TensorFlow, where it is madeavailable astf.keras."
keypoints,"The coordinates of particular features in an image. For example, for animage recognition model that distinguishesflower species, keypoints might be the center of each petal, the stem,the stamen, and so on."
Kernel Support Vector Machines (KSVMs),"A classification algorithm that seeks to maximize the margin betweenpositive andnegative classes by mapping input data vectorsto a higher dimensional space.  For example, consider a classificationproblem in which the input datasethas a hundred features. To maximize the margin betweenpositive and negative classes, a KSVM could internally map those features intoa million-dimension space.  KSVMs uses a loss function calledhinge loss."
k-means,A popular clustering algorithm that groups examplesin unsupervised learning. 
k-median,A clustering algorithm closely related to k-means. 
L1 loss,Loss function based on the absolute value of the differencebetween the values that a model is predicting and the actual values ofthe labels. L1 loss is less sensitive to outliersthan L2 loss.
L1 regularization,"A type of regularization that penalizes weightsin proportion to the sum of the absolute values of the weights. In modelsrelying on sparse features, L1regularization helps drive the weights of irrelevant or barely relevantfeatures to exactly 0, which removes those features from the model.Contrast with L2 regularization."
L2 loss,See squared loss.
L2 regularization,A type of regularization that penalizes weightsin proportion to the sum of the squares of the weights.L2 regularization helps drive outlier weights (those withhigh positive or low negative values) closer to 0 but not quite to 0.(Contrast with L1 regularization.)L2 regularization always improves generalization in linear models.
label,"In supervised learning, the ""answer"" or ""result"" portion of anexample. Each example in a labeled dataset consists of one ormore features and a label. For instance, in a housing dataset, the featuresmight include the number of bedrooms, the number of bathrooms, and the ageof the house, while the label might be the house's price.In a spam detection dataset, the features might include the subject line, thesender, and the email message itself, while the label would probably be either""spam"" or ""not spam."""
labeled example,"An example that contains features and alabel. In supervised training, models learn from labeledexamples."
lambda,Synonym for regularization rate.
landmarks,Synonym for keypoints.
layer,"A set of neurons in aneural network that process a set of inputfeatures, or the output of those neurons."
Layers API (tf.layers),A TensorFlow API for constructing a deep neural networkas a composition of layers. 
learning rate,"A scalar used to train a model via gradient descent. During each iteration,the gradient descent algorithm multiplies thelearning rate by the gradient.  The resulting product is called thegradient step."
least squares regression,A linear regression model trained by minimizingL2 Loss.
linear model,"A model that assigns one weight perfeature to make predictions.(Linear models also incorporate a bias.) By contrast,the relationship of weights to features in  deep modelsis not one-to-one."
linear regression,"Using the raw output (y') of a linear model as theactual prediction in a regression model. The goal ofa regression problem is to make a real-valued prediction. For example, ifthe raw output (y') of a linear model is 8.37, then the prediction is8.37."
logistic regression,A classification model that uses asigmoid function to converta linear model's raw prediction (\(y'\)) into avalue between 0 and 1. 
logits,"The vector of raw (non-normalized) predictions that a classificationmodel generates, which is ordinarily then passed to a normalization function.If the model is solving a multi-class classificationproblem, logits typically become an input to thesoftmax function.The softmax function then generates a vector of (normalized)probabilities with one value for each possible class."
Log Loss,The loss function used in binarylogistic regression.
log-odds,The logarithm of the odds of some event.
Long Short-Term Memory (LSTM),"A type of cell in arecurrent neural network used to processsequences of data in applications such as handwriting recognition, machinetranslation, and image captioning. LSTMs address thevanishing gradient problem that occurs whentraining RNNs due to long data sequences by maintaining history in aninternal memory state based on new input and context from previous cellsin the RNN."
loss,"A measure of how far a model's predictions are from itslabel. Or, to phrase it more pessimistically, a measure ofhow bad the model is. To determine this value, a model must define a lossfunction. For example, linear regression models typically usemean squared error for a loss function,while logistic regression models use Log Loss."
loss curve,A graph of loss as a function of trainingiterations. 
loss surface,A graph of weight(s) vs. loss. Gradient descent aimsto find the weight(s) for which the loss surface is at a local minimum.
LSTM,Abbreviation for Long Short-Term Memory.
machine learning,A program or system that builds (trains) a predictive model from input data.The system uses the learned model to make useful predictions from new(never-before-seen) data drawn from the same distribution as the one used totrain the model. Machine learning also refers to the field of study concernedwith these programs or systems.
majority class,"The more common label in aclass-imbalanced dataset. For example,given a dataset containing 99% non-spam labels and 1% spam labels, thenon-spam labels are the majority class."
Markov decision process (MDP),"A graph representing the decision-making model where decisions(or actions) are taken to navigate a sequence ofstates under the assumption that theMarkov property holds. In reinforcement learning,these transitions between states return a numerical reward."
Markov property,"A property of certain environments, where statetransitions are entirely determined by information implicit in thecurrent state and the agent’s action."
matplotlib,An open-source Python 2D plotting library.matplotlib helps you visualizedifferent aspects of machine learning.
matrix factorization,"In math, a mechanism for finding the matrices whose dot product approximates atarget matrix."
Mean Absolute Error (MAE),"An error metric calculated by taking an average of absolute errors.In the context of evaluating a model’s accuracy, MAE is the averageabsolute difference between the expected and predicted values acrossall training examples."
Mean Squared Error (MSE),The average squared loss per example. MSE is calculated by dividing thesquared loss by the number ofexamples. The values thatTensorFlow Playground displays for"Training loss" and "Test loss" are MSE.
metric,A number that you care about. May or may not be directly optimized in amachine-learning system. A metric that your system tries to optimize iscalled an objective.
Metrics API (tf.metrics),"A TensorFlow API for evaluating models. For example, tf.metrics.accuracydetermines how often a model's predictions match labels. When writing acustom Estimator, you invoke Metrics API functions tospecify how your model should be evaluated."
mini-batch,"A small, randomly selected subset of the entire batch ofexamples run together in a single iteration of trainingor inference. The batch size of a mini-batch is usuallybetween 10 and 1,000. It is much more efficient to calculate the loss on amini-batch than on the full training data."
mini-batch stochastic gradient descent (SGD),"A gradient descent algorithm that usesmini-batches. In other words, mini-batch SGD estimates thegradient based on a small subset of the training data. Vanilla SGDuses a mini-batch of size 1."
minimax loss,"A loss function forgenerative adversarial networks,based on the cross-entropy between the distributionof generated data and real data."
minority class,"The less common label in aclass-imbalanced dataset. For example,given a dataset containing 99% non-spam labels and 1% spam labels, thespam labels are the minority class."
ML,Abbreviation for machine learning.
MNIST,"A public-domain dataset compiled by LeCun, Cortes, and Burges containing60,000 images, each image showing how a human manually wrote a particulardigit from 0–9.  Each image is stored as a 28x28 array of integers, whereeach integer is a grayscale value between 0 and 255, inclusive."
model,The representation of what a machine learning system has learned fromthe training data. 
model capacity,"The complexity of problems that a model can learn. The more complex theproblems that a model can learn, the higher the model’s capacity. A model’scapacity typically increases with the number of model parameters. For aformal definition of classifier capacity, seeVC dimension."
model function,"The function within an Estimator that implementsmachine learning training, evaluation, and inference. For example, thetraining portion of a model function might handle tasks such as definingthe topology of a deep neural network and identifying itsoptimizer function.  When usingpremade Estimators, someone hasalready written the model function for you.  When usingcustom Estimators, you must write the modelfunction yourself."
model training,The process of determining the best model.
Momentum,"A sophisticated gradient descent algorithm in which a learning step dependsnot only on the derivative in the current step, but also on the derivativesof the step(s) that immediately preceded it. Momentum involves computing anexponentially weighted moving average of the gradients over time, analogousto momentum in physics.  Momentum sometimes prevents learning from gettingstuck in local minima."
multi-class classification,"Classification problems that distinguish among more than two classes. Forexample, there are approximately 128 species of maple trees, so a modelthat categorized maple tree species would be multi-class. Conversely, amodel that divided emails into only two categories (spam and not spam)would be a binary classification model."
multi-class logistic regression,Using logistic regression inmulti-class classification problems.
multinomial classification,Synonym for multi-class classification.
NaN trap,"When one number in your model becomes aNaNduring training, which causesmany or all other numbers in your model to eventually become a NaN."
natural language understanding,"Determining a user's intentions based on what the user typed or said.For example, a search engine uses natural language understanding todetermine what the user is searching for based on what the user typed or said."
negative class,"In binary classification, one class istermed positive and the other is termed negative. The positive class isthe thing we're looking for and the negative class is the other possibility.For example, the negative class in a medical test might be ""not tumor.""The negative class in an email classifier might be ""not spam.""See also positive class."
neural network,"A model that, taking inspiration from the brain, is composed of layers(at least one of which is hidden) consisting ofsimple connected units or neurons followed by nonlinearities."
neuron,"A node in a neural network, typically taking inmultiple input values and generating one output value. The neuron calculatesthe output value by applying anactivation function (nonlinear transformation)to a weighted sum of input values."
N-gram,"An ordered sequence of N words.  For example, truly madly is a 2-gram. Becauseorder is relevant, madly truly is a different 2-gram than truly madly."
NLU,Abbreviation for natural languageunderstanding.
node (neural network),A neuron in a hidden layer.
node (TensorFlow graph),An operation in a TensorFlow graph.
noise,"Broadly speaking, anything that obscures the signal in a dataset. Noisecan be introduced into data in a variety of ways. "
non-response bias,See selection bias.
normalization,"The process of converting an actual range of values into a standard rangeof values, typically -1 to +1 or 0 to 1. For example, suppose the naturalrange of a certain feature is 800 to 6,000. Through subtraction and division,you can normalize those values into the range -1 to +1."
numerical data,"Features represented as integers or real-valued numbers.For example, in a real estate model, you would probably represent the sizeof a house (in square feet or square meters) as numerical data.  Representinga feature as numerical data indicates that the feature's values havea mathematical relationship to each other and possibly to the label.For example, representing the size of a house as numerical data indicatesthat a 200 square-meter house is twice as large as a 100 square-meter house.Furthermore, the number of square meters in a house probably has somemathematical relationship to the price of the house."
NumPy,An open-source math librarythat provides efficient array operations in Python.pandas is built on NumPy.
objective,A metric that your algorithm is trying to optimize.
objective function,"The mathematical formula or metric that a model aims to optimize.For example, the objective function forlinear regression is usuallysquared loss. Therefore, when training alinear regression model, the goal is to minimize squared loss."
offline inference,"Generating a group of predictions, storing thosepredictions, and then retrieving those predictions on demand. Contrastwith online inference."
one-hot encoding,"One-hot encoding is commonly used to represent strings or identifiers that have a finite set of possible values. For example, suppose a given botany dataset chronicles 15,000 different species, each denoted with a unique string identifier. As part of feature engineering, you'll probably encode those string identifiers as one-hot vectors in which the vector has a size of 15,000."
one-shot learning,"A machine learning approach, often used for object classification,designed to learn effective classifiers from a single training example."
one-vs.-all,"Given a classification problem with N possible solutions, a one-vs.-allsolution consists of N separatebinary classifiers—one binary classifier foreach possible outcome. "
online inference,Generating predictions on demand. Contrast withoffline inference.
Operation (op),"A node in the TensorFlow graph. In TensorFlow, any procedure that creates,manipulates, or destroys a Tensor is an operation. Forexample, a matrix multiply is an operation that takes two Tensors asinput and generates one Tensor as output."
optimizer,A specific implementation of the gradient descentalgorithm. TensorFlow's base class for optimizers istf.train.Optimizer.
out-group homogeneity bias,"The tendency to see out-group members as more alike than in-group memberswhen comparing attitudes, values, personality traits, and othercharacteristics. In-group refers to people you interact with regularly;out-group refers to people you do not interact with regularly. If youcreate a dataset by asking people to provide attributes aboutout-groups, those attributes may be less nuanced and more stereotypedthan attributes that participants list for people in their in-group."
outliers,Values distant from most other values. 
output layer,The "final" layer of a neural network. The layer containing the answer(s).
overfitting,Creating a model that matches the training data soclosely that the model fails to make correct predictions on new data.
pandas,"A column-oriented data analysis API. Many machine learning frameworks,including TensorFlow, support pandas data structures as input. See thepandas documentationfor details."
parameter,"A variable of a model that the machine learning system trains on its own.For example, weights are parameters whose values themachine learning system gradually learns through successive trainingiterations. Contrast with hyperparameter."
Parameter Server (PS),A job that keeps track of a model's parameters in adistributed setting.
parameter update,"The operation of adjusting a model's parameters duringtraining, typically within a single iteration ofgradient descent."
partial derivative,"A derivative in which all but one of the variables is considered a constant.For example, the partial derivative of f(x, y) with respect to x is thederivative of f considered as a function of x alone (that is, keeping yconstant). The partial derivative of f with respect to x focuses only onhow x is changing and ignores all other variables in the equation."
participation bias,Synonym for non-response bias.  See selection bias.
partitioning strategy,The algorithm by which variables are divided acrossparameter servers.
perceptron,"A system (either hardware or software) that takes in one or more input values,runs a function on the weighted sum of the inputs, and computes a singleoutput value. In machine learning, the function is typically nonlinear, such asReLU, sigmoid, or tanh.For example, the following perceptron relies on the sigmoid function to processthree input values:"
performance,"The traditional meaning within software engineering. Namely: How fast (or efficiently) does this piece of software run?
The meaning within machine learning. Here, performance answers the following question: How correct is this model? That is, how good are the model's predictions?"
perplexity,"One measure of how well a model is accomplishing its task.For example, suppose your task is to read the first few letters of a worda user is typing on a smartphone keyboard, and to offer a list of possiblecompletion words. Perplexity, P, for this task is approximately the numberof guesses you need to offer in order for your list to contain the actualword the user is trying to type."
pipeline,"The infrastructure surrounding a machine learning algorithm. A pipelineincludes gathering the data, putting the data into training data files,training one or more models, and exporting the models to production."
policy,"In reinforcement learning, an agent's probabilistic mappingfrom states to actions."
pooling,"Reducing a matrix (or matrices) created by an earlierconvolutional layer to a smaller matrix.Pooling usually involves taking either the maximum or average valueacross the pooled area. For example, suppose we have thefollowing 3x3 matrix:"
positive class,"In binary classification, the two possibleclasses are labeled as positive and negative. The positive outcome is thething we're testing for. (Admittedly, we're simultaneously testing forboth outcomes, but play along.) For example, the positive class in amedical test might be ""tumor."" The positive class in an email classifiermight be ""spam."""
post-processing,"For example, one might apply post-processing to a binary classifierby setting a classification threshold such thatequality of opportunity is maintainedfor some attribute by checking that the true positive rateis the same for all values of that attribute."
PR AUC (area under the PR curve),"Area under the interpolatedprecision-recall curve, obtained by plotting(recall, precision) points for different values of theclassification threshold. Depending on howit's calculated, PR AUC may be equivalent to theaverage precision of the model."
precision,A metric for classification models. Precisionidentifies the frequency with which a model was correct when predicting thepositive class. 
precision-recall curve,A curve of precision vs. recall at differentclassification thresholds.
prediction,A model's output when provided with an input example.
prediction bias,A value indicating how far apart the average ofpredictions is from the average of labelsin the dataset.
predictive parity,"A fairness metric that checks whether,for a given classifier, the precision ratesare equivalent for subgroups under consideration."
predictive rate parity,Another name for predictive parity.
premade Estimator,"An Estimator that someone has already built.TensorFlow provides several premade Estimators, including DNNClassifier,DNNRegressor, and LinearClassifier.  To learn more aboutpremade Estimators, see thePremade Estimators chapter in the TensorFlow Programmers Guide."
preprocessing,"Processing data before it's used to train a model. Preprocessing could be as simple as removing words from an English text corpus that don't occur in the English dictionary, or could be as complex as re-expressing data points in a way that eliminates as many attributes that are correlated with sensitive attributes as possible. Preprocessing can help satisfy fairness constraints.
"
pre-trained model,"Models or model components (such as embeddings) that havebeen already been trained. Sometimes, you'll feed pre-trained embeddingsinto a neural network. Other times, your model willtrain the embeddings itself rather than rely on the pre-trained embeddings."
prior belief,"What you believe about the data before you begin training on it. Forexample, L2 regularization relies ona prior belief that weights should be small and normallydistributed around zero."
proxy (sensitive attributes),"An attribute used as a stand-in for a sensitive attribute. For example, an individual's postal code might be used as a proxy for their income, race, or ethnicity."
proxy labels,Data used to approximate labels not directly available in a dataset.
Q-function,"In reinforcement learning, the function that predicts the expectedreturn from taking an action in astate and then following a given policy."
Q-learning,"In reinforcement learning, an algorithm that allows an agentto learn the optimal Q-function of aMarkov decision process by applying theBellman equation. The Markov decision process modelsan environment."
quantile,Each bucket in quantile bucketing.
quantile bucketing,"Distributing a feature's values into buckets so that eachbucket contains the same (or almost the same) number of examples.  For example,the following figure divides 44 points into 4 buckets, each of whichcontains 11 points.  In order for each bucket in the figure to contain thesame number of points, some buckets span a different width of x-values."
quantization,An algorithm that implements quantile bucketing ona particular feature in a dataset.
queue,A TensorFlow Operation that implements a queue datastructure. Typicallyused in I/O.
random forest,An ensemble approach to finding the decision tree thatbest fits the training data by creating many decision trees and thendetermining the "average" one. The "random" part of the term refers tobuilding each of the decision trees from a random selection of features;the "forest" refers to the set of decision trees.
random policy,"In reinforcement learning, a policy that chooses anaction at random."
rank (ordinality),"The ordinal position of a class in a machine learning problem that categorizesclasses from highest to lowest. For example, a behavior rankingsystem could rank a dog's rewards from highest (a steak) tolowest (wilted kale)."
rank (Tensor),"The number of dimensions in a Tensor. For instance,a scalar has rank 0, a vector has rank 1, and a matrix has rank 2."
rater,A human who provides labels in examples.Sometimes called an "annotator."
recall,"A metric for classification models that answersthe following question: Out of all the possible positive labels, how manydid the model correctly identify? That is:"
recommendation system,"A system that selects for each user a relatively small set of desirableitems from a large corpus.For example, a video recommendation system might recommend two videosfrom a corpus of 100,000 videos, selecting Casablanca andThe Philadelphia Story for one user, and Wonder Woman andBlack Panther for another. "
Rectified Linear Unit (ReLU),"An activation function with the following rules: If input is negative or zero, output is 0.
If input is positive, output is equal to input."
recurrent neural network,"A neural network that is intentionally run multipletimes, where parts of each run feed into the next run. Specifically,hidden layers from the previous run provide part of theinput to the same hidden layer in the next run. Recurrent neural networksare particularly useful for evaluating sequences, so that the hidden layerscan learn from previous runs of the neural network on earlier parts ofthe sequence."
regression model,"A type of model that outputs continuous (typically, floating-point) values.Compare with classification models, whichoutput discrete values, such as ""day lily"" or ""tiger lily."""
regularization,The penalty on a model's complexity. Regularization helps preventoverfitting. 
regularization rate,"A scalar value, represented as lambda, specifying the relative importanceof the regularization function. The following simplified lossequation shows the regularization rate's influence:"
reinforcement learning (RL),"A family of algorithms that learn an optimal policy, whose goalis to maximize return when interacting withan environment.For example, the ultimate reward of most games is victory.Reinforcement learning systems can become expert at playing complexgames by evaluating sequences of previous game moves that ultimatelyled to wins and sequences that ultimately led to losses."
replay buffer,"In DQN-like algorithms, the memory used by the agentto store state transitions for use inexperience replay."
reporting bias,"The fact that the frequency with which people write about actions,outcomes, or properties is not a reflection of their real-worldfrequencies or the degree to which a property is characteristicof a class of individuals. Reporting bias can influence the compositionof data that machine learning systems learn from."
representation,The process of mapping data to useful features.
re-ranking,"The final stage of a recommendation system,during which scored items may be re-graded according to some other(typically, non-ML) algorithm. "
return,"In reinforcement learning, given a certain policy and a certain state, thereturn is the sum of all rewards that the agentexpects to receive when following the policy from thestate to the end of the episode. The agentaccounts for the delayed nature of expected rewards by discounting rewardsaccording to the state transitions required to obtain the reward."
reward,"In reinforcement learning, the numerical result of taking an action in a state, as defined by the environment."
ridge regularization,"Synonym for L2 regularization. The termridge regularization is more frequently used in pure statisticscontexts, whereas L2 regularization is used more oftenin machine learning."
RNN,Abbreviation for recurrent neural networks.
ROC (receiver operating characteristic) Curve,A curve of true positive rate vs.false positive rate at differentclassification thresholds. See alsoAUC.
root directory,The directory you specify for hosting subdirectories of the TensorFlowcheckpoint and events files of multiple models.
Root Mean Squared Error (RMSE),The square root of the Mean Squared Error.
rotational invariance,"In an image classification problem, an algorithm's ability to successfullyclassify images even when the orientation of the image changes. For example,the algorithm can still identify a tennis racket whether it is pointing up,sideways, or down. Note that rotational invariance is not always desirable;for example, an upside-down 9 should not be classified as a 9."
SavedModel,"The recommended format for saving and recovering TensorFlow models. SavedModelis a language-neutral, recoverable serialization format, which enableshigher-level systems and tools to produce, consume, and transform TensorFlowmodels."
Saver,A TensorFlow objectresponsible for saving model checkpoints.
scalar,A single number or a single string that can be represented as atensor of rank 0.
scaling,"A commonly used practice in feature engineeringto tame a feature's range of values to match the range of other features inthe dataset. For example, suppose that you want all floating-point featuresin the dataset to have a range of 0 to 1. Given a particular feature'srange of 0 to 500, you could scale that feature by dividing each valueby 500."
scikit-learn,A popular open-source machine learning platform. Seewww.scikit-learn.org.
scoring,The part of a recommendation system thatprovides a value or ranking for each item produced by thecandidate generation phase.
selection bias,Errors in conclusions drawn from sampled data due to a selection processthat generates systematic differences between samples observed in the dataand those not observed.  The following forms of selection bias exist:
semi-supervised learning,"Training a model on data where some of the training examples have labels butothers don’t. One technique for semi-supervised learning is to infer labels forthe unlabeled examples, and then to train on the inferred labels to create a newmodel. Semi-supervised learning can be useful if labels are expensive to obtainbut unlabeled examples are plentiful."
sensitive attribute,"A human attribute that may be given special consideration for legal, ethical, social, or personal reasons."
sentiment analysis,"Using statistical or machine learning algorithms to determine a group'soverall attitude—positive or negative—toward a service, product,organization, or topic. For example, usingnatural language understanding,an algorithm could perform sentiment analysis on the textual feedbackfrom a university course to determine the degree to which studentsgenerally liked or disliked the course."
sequence model,"A model whose inputs have a sequential dependence. For example, predictingthe next video watched from a sequence of previously watched videos."
serving,A synonym for inferring.
session (tf.session),"An object that encapsulates the state of the TensorFlow runtimeand runs all or part of a graph. When using thelow-level TensorFlow APIs, you instantiate and manage one or moretf.session objects directly. When using the Estimators API,Estimators instantiate session objects for you."
shape (Tensor),The number of elements in each dimension of atensor. The shape is represented as a list of integers.
sigmoid function,"A function that maps logistic or multinomial regression output (log odds) toprobabilities, returning a value between 0 and 1.  "
similarity measure,"In clustering algorithms, the metric used to determinehow alike (how similar) any two examples are."
size invariance,"In an image classification problem, an algorithm's ability to successfullyclassify images even when the size of the image changes. For example,the algorithm can still identify acat whether it consumes 2M pixels or 200K pixels. Note that even the bestimage classification algorithms still have practical limits on size invariance.For example, an algorithm (or human) is unlikely to correctly classify acat image consuming only 20 pixels."
sketching,"In unsupervised machine learning,a category of algorithms that perform a preliminary similarity analysison examples. Sketching algorithms use alocality-sensitive hash functionto identify points that are likely to be similar, and then groupthem into buckets."
softmax,"A function that provides probabilities for each possible class in amulti-class classification model. The probabilities add upto exactly 1.0. For example, softmax might determine that the probability of aparticular image being a dog at 0.9, a cat at 0.08, and a horse at 0.02.(Also called full softmax.)"
sparse feature,"Feature vector whose values are predominately zero or empty.For example, a vector containing a single 1 value and a million 0 values issparse. As another example, words in a search query could also be asparse feature—there are many possible words in a given language, but only afew of them occur in a given query."
sparse representation,A representation of a tensor that only storesnonzero elements.
sparse vector,A vector whose values are mostly zeroes. See also sparsefeature.
sparsity,The number of elements set to zero (or null) in a vector or matrix dividedby the total number of entries in that vector or matrix. 
spatial pooling,See pooling.
squared hinge loss,The square of the hinge loss.  Squared hinge loss penalizesoutliers more harshly than regular hinge loss.
squared loss,"The loss function used inlinear regression.  (Also known asL2 Loss.) This function calculates the squares ofthe difference between a model's predicted value for a labeledexample and the actual value of the label.Due to squaring, this loss function amplifies the influence of bad predictions.That is, squared loss reacts more strongly to outliersthan L1 loss."
state,"In reinforcement learning, the parameter values that describe the currentconfiguration of the environment, which the agent uses tochoose an action."
state-action value function,Synonym for Q-function.
static model,A model that is trained offline.
stationarity,"A property of data in a dataset, in which the data distribution stays constantacross one or more dimensions. Most commonly, that dimension is time, meaningthat data exhibiting stationarity doesn't change over time. For example, datathat exhibits stationarity doesn't change from September to December."
step,A forward and backward evaluation of one batch.
step size,Synonym for learning rate.
stochastic gradient descent (SGD),"A gradient descent algorithm in which the batch sizeis one. In other words, SGD relies on a single example chosen uniformly atrandom from a dataset to calculate an estimate of the gradient at each step."
stride,"In a convolutional operation or pooling, the delta in each dimension of thenext series of input slices. For example, the following animationdemonstrates a (1,1) stride during a convolutional operation. Therefore,the next input slice starts one position to the right of the previous inputslice. When the operation reaches the right edge, the next slice is allthe way over to the left but one position down."
structural risk minimization (SRM),"The desire to build the most predictive model (for example, lowest loss).
The desire to keep the model as simple as possible (for example, strong regularization)."
subsampling,See pooling.
summary,"In TensorFlow, a value or set of values calculated at a particularstep, usually used for tracking model metrics during training."
supervised machine learning,"Training a model from input data and its correspondinglabels. Supervised machine learning is analogous to a studentlearning a subject by studying a set of questions and their correspondinganswers.  After mastering the mapping between questions and answers, thestudent can then provide answers to new (never-before-seen) questions onthe same topic.  Compare withunsupervised machine learning."
synthetic feature,"A feature not present among the input features, butcreated from one or more of them. "
tabular Q-learning,"In reinforcement learning, implementing Q-learningby using a table to store the Q-functions for everycombination of state and action."
target,Synonym for label.
target network,"In Deep Q-learning, a neural network that is a stableapproximation of the main neural network, where the main neural networkimplements either a Q-function or a policy.Then, you can train the main network on the Q-values predicted by the targetnetwork. Therefore, you prevent the feedback loop that occurs when the mainnetwork trains on Q-values predicted by itself. By avoiding this feedback,training stability increases."
temporal data,"Data recorded at different points in time. For example, winter coat salesrecorded for each day of the year would be temporal data."
Tensor,"The primary data structure in TensorFlow programs. Tensors are N-dimensional(where N could be very large) data structures, most commonly scalars, vectors,or matrices. The elements of a Tensor can hold integer, floating-point,or string values."
TensorBoard,The dashboard that displays the summaries saved during the execution of one ormore TensorFlow programs.
TensorFlow,"A large-scale, distributed, machine learning platform. The term also refers tothe base API layer in the TensorFlow stack, which supports general computationon dataflow graphs."
TensorFlow Playground,A program that visualizes how differenthyperparameters influence model(primarily neural network) training. Go tohttp://playground.tensorflow.orgto experiment with TensorFlow Playground.
TensorFlow Serving,A platform to deploy trained models in production.
Tensor Processing Unit (TPU),An application-specific integrated circuit (ASIC) that optimizes theperformance of machine learning workloads. These ASICs are deployed asmultiple TPU chips on a TPU device.
Tensor rank,See rank (Tensor).
Tensor shape,"The number of elements a Tensor contains in various dimensions.For example, a [5, 10] Tensor has a shape of 5 in one dimension and 10in another."
Tensor size,"The total number of scalars a Tensor contains. For example, a[5, 10] Tensor has a size of 50."
termination condition,"In reinforcement learning, the conditions that determine when anepisode ends, such as when the agent reaches a certain stateor exceeds a threshold number of state transitions.For example, in tic-tac-toe (alsoknown as noughts and crosses), an episode terminates either when a player marksthree consecutive spaces or when all spaces are marked."
test set,The subset of the dataset that you use to test your modelafter the model has gone through initial vetting by the validation set.
tf.Example,A standardprotocol bufferfor describing input data for machine learning model training or inference.
tf.keras,An implementation of Keras integrated intoTensorFlow.
time series analysis,"A subfield of machine learning and statistics that analyzestemporal data.  Many types of machine learningproblems require time series analysis, including classification, clustering,forecasting, and anomaly detection. For example, you could usetime series analysis to forecast the future sales of winter coats by monthbased on historical sales data."
timestep,One "unrolled" cell within arecurrent neural network.
tower,"A component of a deep neural network thatis itself a deep neural network without an output layer. Typically,each tower reads from an independent data source. Towers are independentuntil their output is combined in a final layer."
TPU,Abbreviation for Tensor Processing Unit.
TPU chip,A programmable linear algebra accelerator with on-chip high bandwidth memorythat is optimized for machine learning workloads.Multiple TPU chips are deployed on a TPU device.
TPU device,"A printed circuit board (PCB) with multiple TPU chips,high bandwidth network interfaces, and system cooling hardware."
TPU master,"The central coordination process running on a host machine that sends andreceives data, results, programs, performance, and system health informationto the TPU workers. The TPU master also manages the setupand shutdown of TPU devices."
TPU node,A TPU resource on Google Cloud Platform with a specificTPU type. The TPU node connects to yourVPC Network from apeer VPC network.TPU nodes are a resource defined in theCloud TPU API.
TPU Pod,A specific configuration of TPU devices in a Googledata center. All of the devices in a TPU pod are connected to one anotherover a dedicated high-speed network. A TPU Pod is the largest configuration ofTPU devices available for a specific TPU version.
TPU resource,"A TPU entity on Google Cloud Platform that you create, manage, or consume. Forexample, TPU nodes and TPU types areTPU resources."
TPU slice,A TPU slice is a fractional portion of the TPU devices ina TPU Pod. All of the devices in a TPU slice are connectedto one another over a dedicated high-speed network.
TPU type,"A configuration of one or more TPU devices with a specificTPU hardware version. You select a TPU type when you createa TPU node on Google Cloud Platform. For example, a v2-8TPU type is a single TPU v2 device with 8 cores. A v3-2048 TPU type has 256networked TPU v3 devices and a total of 2048 cores. TPU types are a resourcedefined in theCloud TPU API."
TPU worker,A process that runs on a host machine and executes machine learning programson TPU devices.
training,The process of determining the ideal parameters comprisinga model.
training set,The subset of the dataset used to train a model.
trajectory,"In reinforcement learning, a sequence oftuples that representa sequence of state transitions of the agent,where each tuple corresponds to the state, action,reward, and next state for a given state transition."
transfer learning,"Transferring information from one machine learning task to another.For example, in multi-task learning, a single model solves multiple tasks,such as a deep model that has different output nodes fordifferent tasks.  Transfer learning might involve transferring knowledgefrom the solution of a simpler task to a more complex one, or involvetransferring knowledge from a task where there is more data to one wherethere is less data."
translational invariance,"In an image classification problem, an algorithm's ability to successfullyclassify images even when the position of objects within the image changes.For example, the algorithm can still identify a dog, whether it is in thecenter of the frame or at the left end of the frame."
trigram,An N-gram in which N=3.
true negative (TN),"An example in which the model correctly predicted thenegative class. For example, the model inferred thata particular email message was not spam, and that email message really wasnot spam."
true positive (TP),"An example in which the model correctly predicted thepositive class. For example, the model inferred thata particular email message was spam, and that email message really was spam."
true positive rate (TPR),Synonym for recall. That is: TRUE POSITIVE RATE=TRUE POSITIVES/(TURE POSITIVES+FLASE NEGATIVES)
unawareness (to a sensitive attribute),"A situation in which sensitive attributes arepresent, but not included in the training data. Because sensitive attributesare often correlated with other attributes of one’s data, a model trainedwith unawareness about a sensitive attribute could still havedisparate impact with respect to that attribute,or violate other fairness constraints."
underfitting,Producing a model with poor predictive ability because the modelhasn't captured the complexity of the training data. 
unlabeled example,"An example that contains features but no label.Unlabeled examples are the input to inference. Insemi-supervised andunsupervised learning,unlabeled examples are used during training."
unsupervised machine learning,"Training a model to find patterns in a dataset, typically anunlabeled dataset."
upweighting,Applying a weight to the downsampled class equalto the factor by which you downsampled.
user matrix,"In recommendation systems, anembedding generated bymatrix factorizationthat holds latent signals about user preferences.Each row of the user matrix holds information about the relativestrength of various latent signals for a single user.For example, consider a movie recommendation system.  In this system,the latent signals in the user matrix might represent each user's interestin particular genres, or might be harder-to-interpret signals that involvecomplex interactions across multiple factors."
validation,"A process used, as part of training, to evaluatethe quality of a machine learning modelusing the validation set. Because the validationset is disjoint from the training set, validation helps ensure that themodel’s performance generalizes beyond the training set."
validation set,A subset of the dataset—disjoint from the training set—usedin validation.
vanishing gradient problem,"The tendency for the gradients of early hidden layersof some deep neural networks to becomesurprisingly flat (low). Increasingly lower gradients result in increasinglysmaller changes to the weights on nodes in a deep neural network, leading tolittle or no learning. Models suffering from the vanishing gradient problembecome difficult or impossible to train.Long Short-Term Memory cells address this issue."
Wasserstein loss,"One of the loss functions commonly used ingenerative adversarial networks,based on the earth-mover'sdistance betweenthe distribution of generated data and real data."
weight,"A coefficient for a feature in a linear model, or an edgein a deep network. The goal of training a linear model is to determinethe ideal weight for each feature. If a weight is 0, then its correspondingfeature does not contribute to the model."
Weighted Alternating Least Squares (WALS),"An algorithm for minimizing the objective function duringmatrix factorization inrecommendation systems, which allows adownweighting of the missing examples. WALS minimizes the weightedsquared error between the original matrix and the reconstruction byalternating between fixing the row factorization and column factorization.Each of these optimizations can be solved by least squaresconvex optimization. For details, see theRecommendation Systems course"
wide model,"A linear model that typically has manysparse input features. We refer to it as ""wide"" sincesuch a model is a special type of neural network with alarge number of inputs that connect directly to the output node. Wide modelsare often easier to debug and inspect than deep models. Although wide modelscannot express nonlinearities through hidden layers,they can use transformations such asfeature crossing andbucketization to model nonlinearities in different ways."
width,The number of neurons in a particular layerof a neural network.
nlp,"Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data."
Natural language processing,"Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data."
deep learning,"Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised."
